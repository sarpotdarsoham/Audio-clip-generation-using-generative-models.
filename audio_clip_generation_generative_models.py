# -*- coding: utf-8 -*-
"""Audio_clip_generation_generative_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iVOZ1I0jvSWmEsC3EC52wfS4r4r-a1Xo

# DL Assignment 2

## [Notebook Link](https://www.kaggle.com/nileshkumargupta/dl-assignment-2)

<h3>The task is to create a generative model (either a GAN or a VAE) that would generate
an audio clip based on an input parameter that specifies the genre of the audio clip.</h3>

## Prerequisites
"""

''' Import libraries '''
import torch 
import torch.nn as nn

# using librosa for audio processing
import librosa
import librosa.display
import IPython.display as ipd
from IPython.core.display import display
import numpy as np
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data import Dataset

# Check if device is available

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Currently running on", device)

# Hyper Parameters to be used later
SAMPLE_RATE = 16000
LEARNING_RATE = 5e-5
BATCH_SIZE = 128
Z_DIM = 100
NUM_EPOCHS = 5
FEATURES_DISC = 32
FEATURES_GEN = 32
NUM_CLASSES = 10
CRITIC_ITERATIONS = 5
WEIGHT_CLIP = 0.01

"""<h3>Adding and playing audio files</h3>"""

from google.colab import drive
drive.mount('/content/drive')

sample_data, _ = librosa.load('/content/drive/MyDrive/DL Assignment/classical.00000.wav', sr=SAMPLE_RATE)
ipd.Audio(data=sample_data, rate=SAMPLE_RATE)

"""## Approach

### We follow the approach presented in the paper "[Adversarial Audio Synthesis](https://arxiv.org/abs/1802.04208)". In the approach presented there they take the [DCGAN](https://arxiv.org/abs/1511.06434) model which was found to be extremely succesful for generation of images and try to modify it to suit the generation of audio. Most of the architecture of the model is referred to from these two papers and is adjusted to suit the input and output size of our data. We also implement conditional GAN on top of this model to achieve our results. The modification is done in the form of adding an embedding to both the generator and discriminator part of the model. The loss function used is also different from standard DCGAN and is referred from [WGAN](https://arxiv.org/abs/1701.07875). This loss function is preferred as it has been shown that it results in better stability of the training process. The conditional GAN architecture is referred from [here](https://www.youtube.com/watch?v=Hp-jWm2SzR8).

## Dataset and Loading
"""

# A list of all the data samples are formed to be used in data loader

categories = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] 

file_numbers = []
for i in range(10):
  file_numbers.append("0000" + str(i))

for i in range(10, 100):
  file_numbers.append("000" + str(i))

file_names = []

for category in categories:
  for num in file_numbers:
    
    # the data is divided into three chunks of 10s data
    for _ in range(3):
      file_names.append(category + "." + num)

# loading data set

# We form a Custom Dataset object to be used later

class CustomAudioDataset(Dataset):
  def __init__(self, file_dir, file_names, categories):
    super(CustomAudioDataset, self).__init__()
    self.file_dir = file_dir
    self.categories = categories
    self.file_names = file_names

  def __len__(self):
    return len(file_names)

  def __getitem__(self, idx):
    
    try: 
        data, sample_rate = librosa.load(self.file_dir + self.file_names[idx] + ".wav", 16000)
    except:
        print("Failed loading file", self.file_names[idx])
        return torch.zeros(1, 160008).to(device), int (idx/300)
    
    # the input size is set to be 160008, so the data is truncated or padded accordingly
    data = data[:480024]
    data = np.pad(data, [0, 480024 - len(data)])
    
    # the data is split into three parts and stored
    split_data = np.array_split(data, 3)
    data_tensor = torch.from_numpy(split_data[idx % 3]).to(device)
    data_tensor = torch.reshape(data_tensor, (1, 160008))
    return data_tensor, int(idx / 300)

audio_data_set = CustomAudioDataset('/content/drive/MyDrive/DL Assignment/', file_names, categories)
loader = DataLoader(audio_data_set, batch_size=BATCH_SIZE, shuffle=True)

"""## Discriminator Model

### Both Discriminator Model and Generator Model follow the architecture of DCGAN with suggestion from WaveGAN Model

![image.png](attachment:e2be1cee-2810-4132-aa04-a7a453585975.png)

### All the suggestion are followed with some changes to fit the size of data. For the last suggestion, only WGAN loss model was used instead of WGAN-GP
"""

class ConditionalDiscriminator(nn.Module):
  def __init__(self, features_disc, input_size=160008, num_classes=10, embed_size=4000):
    super(ConditionalDiscriminator, self).__init__()
    
    '''
    The architecture follows the DCGAN Model but with some modifications
    '''

    self.input_size = input_size

    # An embed layer is introduced to implemented the conditional GAN, the input is expanded and added as a channel to the original input
    self.embed_layer = nn.Sequential(
        nn.Embedding(num_classes, input_size),
    )

    # The discriminator follows the methodology of DCGAN with changes as mentioned above
    self.disc = nn.Sequential(    #(n, 160008, 1)
        nn.Conv1d( 2, features_disc, 16, 8, 2), # (n, 20000, 32)
        nn.LeakyReLU(0.2),
        self.block(features_disc, features_disc * 2, 16, 8, 2), #(n, 2499, 64)
        self.block(features_disc * 2, features_disc * 4, 16, 8, 2), #(n, 311, 128)
        self.block(features_disc * 4, features_disc * 8, 16, 8, 2), #(n, 38, 256)
        nn.Flatten(),
        nn.Dropout(0.2),
        nn.Linear(38 * 256, 1),
        nn.Sigmoid(),
    )

  def block(self, in_channels, out_channels, kernel_size, stride, padding):
    return nn.Sequential(
        nn.Conv1d(
            in_channels,
            out_channels,
            kernel_size,
            stride,
            padding
        ),
        nn.LeakyReLU(0.2)
    )

  def forward(self, x, label):

    # A normal forward of input is done with embedding of label added as an additional channel
    embed = self.embed_layer(label)
    embed = torch.reshape(embed, (label.shape[0], 1, self.input_size)) 
    x = torch.cat([x, embed], dim=1)

    return self.disc(x)

"""## Generator Model"""

class ConditionalGenerator(nn.Module):
  def __init__(self, z_dim, features_gen, num_classes=10, input_size=160008, embed_size=100):
    super(ConditionalGenerator, self).__init__()
    
    '''
    The architecture follows the DCGAN Model but with some modifications
    '''

    self.embed_size = embed_size

    # The generator follows the methodology of DCGAN with changes as mentioned above
    self.net = nn.Sequential(
        self.block(z_dim + embed_size, features_gen * 16, 39, 4, 0),
        self.block(features_gen * 16, features_gen * 8, 16, 8, 4),
        self.block(features_gen * 8, features_gen * 4, 16, 8, 2),
        self.block(features_gen * 4, features_gen * 2, 16, 8, 2),
        nn.ConvTranspose1d(features_gen * 2, 1, 16, 8, 2),
        nn.Tanh()
    )

    # An embed layer is introduced to implemented the conditional GAN, the input is expanded and added as a channel to the original input
    self.embed_layer = (
        nn.Embedding(num_classes, embed_size)
    )

  def block(self, in_channels, out_channels, kernel_size, stride, padding):
    return nn.Sequential(
        nn.ConvTranspose1d(
            in_channels,
            out_channels,
            kernel_size, 
            stride,
            padding, 
            bias=False
        ),
        nn.ReLU(),
    )

  def forward(self, x, label):
    
    # An embedding of the input is formed and added as an channel to noise
    embedding = self.embed_layer(label)
    embedding = torch.reshape(embedding, (label.shape[0], self.embed_size, 1))
    
    x = torch.cat([x, embedding], dim=1)
    output = self.net(x)
    
    # the output is more than the required dimension so the last few samples are dropped
    return output[:,:,:160008]

"""## Helper Function

### The function here initializes the weight of the three layers. This initailization method is mentioned in DCGAN
"""

def generate_random_sample(model, label):
    label_t = torch.tensor([[label]]).to(device)
    noise = torch.randn(1, Z_DIM, 1).to(device)
    aud = model(noise, label)
    aud = gen_audio.reshape(-1)
    aud = gen_audio.cpu().detach().numpy()
    display(ipd.Audio(aud, rate=SAMPLE_RATE))

def initialize_weights(model):
  for m in model.modules():
    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):
      nn.init.normal_(m.weight.data, 0.0, 0.2)

"""## Defining the Training Function"""

def train_WGAN(loader, lr, batch_size, num_epochs, critic_iter, weight_clip):

  # initialize weights
  initialize_weights(model_gen)
  initialize_weights(model_disc)

  # initialize optimizatioin functions as suggested in WGAN
  opt_gen = optim.RMSprop(model_gen.parameters(), lr=lr)
  opt_disc = optim.RMSprop(model_disc.parameters(), lr=lr)

  # set both model to train
  model_gen.train()
  model_disc.train()

  for epoch in range(num_epochs):
    for batch_idx, (real, label) in enumerate(loader):
      real = real.to(device)
      label = label.to(device)
      noise = torch.randn(real.shape[0], Z_DIM, 1).to(device)
      fake = model_gen(noise, label)

      # Following the WGAN implementaion, the backward_prop of discriminator is done CRITIC_ITERATIONS times more than Generator with a changed loss function
      for _ in range(CRITIC_ITERATIONS):
        noise = torch.randn(real.shape[0], Z_DIM, 1).to(device)
        fake = model_gen(noise, label)
        critic_real = model_disc(real, label).reshape(-1)
        critic_fake = model_disc(fake, label).reshape(-1)
        loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))
        model_disc.zero_grad()
        loss_critic.backward(retain_graph=True)
        opt_disc.step()

        # Clipping of weights
        for p in model_disc.parameters():
          p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)

      output = model_disc(fake, label).reshape(-1)
      loss_gen = -torch.mean(output)
      model_gen.zero_grad()
      loss_gen.backward()
      opt_gen.step()

      # Printing results every 5 batches
      if batch_idx % 5 == 0:
        
        
          print("Epoch", epoch, " batch", batch_idx + 1, " completed.")
          test_noise = torch.randn(1, 100, 1).to(device)
          test_label = torch.ones(1, 1).int().to(device) # As label for classical is 1
          gen_audio = model_gen(test_noise, test_label)
          gen_audio = gen_audio.reshape(-1)
          gen_audio = gen_audio.cpu().detach().numpy()
          display(ipd.Audio(gen_audio, rate=SAMPLE_RATE))

# torch.autograd.set_detect_anomaly(True)

model_gen = ConditionalGenerator(Z_DIM, FEATURES_GEN).to(device)
model_disc = ConditionalDiscriminator(FEATURES_DISC).to(device)

train_WGAN(loader, LEARNING_RATE, BATCH_SIZE, NUM_EPOCHS, CRITIC_ITERATIONS, WEIGHT_CLIP)

generate_random_sample(model_gen, 1)

"""### The results obtained were not satisfactory and the model almost always output static noise. Possible changes that we think might work are Hyperparmeter tuning, Implementing Gradient Penalty, Looking at other representations of audio data such as MFCC.

<h4>
    Group Members
    
    Nilesh Kumar Gupta -> GAN Implementaion
    Soham Sachin Sarpotdar -> Helper Functions
    Atishay Jain -> Preprocessing
    
</h4>
"""

